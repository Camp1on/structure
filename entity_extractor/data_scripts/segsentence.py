# -*- coding: utf-8 -*-
# @Time    : 2021/10/6 14:20
# @Author  : heyee (jialeyang)


from math import log
from structure.entity_extractor.data_scripts.data_utils import *


class SegSentence(object):
    def __init__(self):
        self.seg_tags_level_0 = ['\t']
        self.seg_tags_level_1 = ['\t', 'ã€‚', ',', 'ï¼Œ', "|", "ï¼›", ";", '!', 'ï¼', '?', 'ï¼Ÿ', 'â€¦', 'ã€', 'ï½']
        # self.seg_tags_level_1 = ['ã€‚', ',', 'ï¼Œ']
        # self.seg_tags_level_0 = ['\t', '\n', '\r', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ', ';', 'ï¼›', ',', 'ï¼Œ']
        # self.seg_tags_level_1 = ['.', 'â€¦', 'ã€', ':', 'ï¼š', '~', '\uFF5E', ' ', '\u00A0', '#', '-']
        # self.seg_tags_level_2 = ['#', '-']
        self.FREQ = {}
        self.total = 0

    def cut(self, sentence):
        """
        old:
            (1) é‡åˆ° level_0 çº§åˆ«çš„æ ‡ç‚¹ç¬¦å·ç›´æ¥è¿›è¡Œæ–­å¥å¤„ç†ï¼Œå¯¹æ–­å¥åé•¿åº¦å¤§äº 60 çš„å­å¥è¿›è¡Œæ­¥éª¤ï¼ˆ2ï¼‰
            (2) ç”¨ jieba åˆ†è¯æ€æƒ³ï¼ˆåå‘åŠ¨æ€è§„åˆ’+å‰å‘è´ªå¿ƒç®—æ³•ï¼‰å¯¹å¥å­é•¿å¥å»ºæ¨¡ï¼Œä»¥æœ€ç»ˆå¾—åˆ°çš„æœ€å¤§æ¦‚ç‡è·¯å¾„æœ€ä¸ºæ–­å¥ç»“æœ
            (3) å¯¹æ­¥éª¤ï¼ˆ2ï¼‰ä¸­é•¿å¥ä»å¤§äº 60 çš„å­å¥è¿›è¡Œæ­¤æ­¥å¤„ç†
        new:
            (1) ä¼˜å…ˆå¯¹"\t"è¿›è¡Œæ–­å¥ï¼Œæ–­å¥åé•¿åº¦å¤§äº120çš„ï¼Œå†ç»†ç²’åº¦æ–­å¥
        """
        # step 1
        sen_collect = []
        # split_level0_list = self.cut_helper(self.seg_tags_level_0, sentence)

        # for idx, val in enumerate(split_level0_list):
        #     if len(val) < 60:
        #         sen_collect.append(val)
        #     else:
        #         split_level1_list = self.toSentenceListHelper(val)
        #         sen_collect.extend(split_level1_list)

        split_level0_list = self.cut_helper(self.seg_tags_level_0, sentence)
        self.total = len(split_level0_list)
        res = self.cut_DAG(split_level0_list)

        for idx, val in enumerate(res):
            if len(val) < 80:
                sen_collect.append(val)
                if len(val) < 10:
                    print(res)
                    continue
            else:
                split_level1_list = self.toSentenceListHelper(val)
                self.total = len(split_level1_list)
                res_level1 = self.cut_DAG(split_level1_list, min_len=40)
                sen_collect.extend(res_level1)
                for i in res_level1:
                    if len(i) < 10 or len(i) > 120:
                        print(res_level1)
                        break

        return sen_collect

        # filter_level0_list = [[idx, val] for idx, val in enumerate(split_level0_list) if len(val) > 60]
        #
        # # step 2
        # for idx, val in filter_level0_list:
        #     self.total = len(val)
        #     val_split_level1 = self.toSentenceListHelper(val)
        #     res = self.cut_DAG(val_split_level1)
        #     split_level0_list[idx] = res
        #
        # # flat list of list to list
        # split_level0_list = [[w] if isinstance(w, str) else w for w in split_level0_list]
        # if any(isinstance(el, list) for el in split_level0_list):
        #     flat_list = [item for sublist in split_level0_list for item in sublist]
        # else:
        #     flat_list = split_level0_list
        # return flat_list

    def get_DAG(self, sentence, min_len):
        self.FREQ = self.check_initialized(sentence)
        DAG = {}
        N = len(sentence)
        for k in range(N):
            tmplist = []
            tmpsum = 0
            i = k
            while i < N and tmpsum < 80:
                tmpsum += self.FREQ[i]
                if min_len <= tmpsum < 80:
                    tmplist.append(i)
                elif i == N - 1 and tmpsum < 40:
                    tmplist.append(i)
                i += 1
            if not tmplist:
                tmplist.append(k)
            DAG[k] = tmplist
        return DAG

    def cut_DAG(self, sentence, min_len=20):
        DAG = self.get_DAG(sentence, min_len=min_len)
        route = {}
        # è‡ªåº•å‘ä¸Šï¼Œåå‘åŠ¨æ€è§„åˆ’
        self.calc(sentence, DAG, route)
        # å‰å‘è´ªå¿ƒç®—æ³•æœç´¢
        res = []
        x = 0
        N = len(sentence)
        while x < N:
            y = route[x][1] + 1
            res.append("".join(sentence[x:y]))
            x = y

        return res

    def calc(self, sentence, DAG, route):
        N = len(sentence)
        route[N] = (0, 0)
        logtotal = log(self.total)
        for idx in range(N - 1, -1, -1):
            route[idx] = max((log(self.calc_helper(idx, x) or 1) -
                              logtotal + route[x + 1][0], x) for x in DAG[idx])

    def calc_helper(self, idx, x):
        return sum([self.FREQ.get(i) for i in range(idx, x + 1)])

    def cut_helper(self, seg_tags_level, sentence):
        tag = "([" + "".join(seg_tags_level) + "])"
        split_list = re.split(tag, sentence)
        # å°†æ ‡ç‚¹ä¸å½“å‰å¥å­æ‹¼æ¥åœ¨ä¸€èµ·
        split_list.append("")
        split_res = ["".join(i) for i in zip(split_list[0::2], split_list[1::2])]

        return split_res

    def toSentenceListHelper(self, sentence):
        sentences = []
        chars = list(sentence)
        preLenth = 0
        sb = ""
        for i in range(len(chars)):
            if len(sb) == 0 and (chars[i].isspace() or chars[i] == " "):
                continue
            preLenth += 1
            sb += chars[i]
            if chars[i] == " ":
                # if i < len(chars) - 1 and ord(chars[i + 1]) > 128:
                if i < len(chars) - 1 and preLenth > 15:
                    sentences.append(sb)
                    sb = ""
                    preLenth = 0
            elif chars[i] in self.seg_tags_level_1:
                sentences.append(sb)
                sb = ""
                preLenth = 0
        if len(sb) > 0:
            sentences.append(sb)
        return sentences

    def check_initialized(self, sentence_list):
        """
        ç”¨äºè¿”å› sentence_list ä¸­æ¯ä¸ª sentence çš„é•¿åº¦

        :param sentence_list:
        :return: dict
        """
        sen_dict = {}
        for idx, val in enumerate(sentence_list):
            sen_dict[idx] = len(val)
        return sen_dict

    def fetch_data_from_log(self, path):
        with open(path, 'r') as f:
            text = f.read()

        return text

    def analysis_data(self, text):
        """
        å¯¹é•¿åº¦å¤§äº 60 å°±ç›´æ¥ä¸¢å¼ƒ vs åŸºäºè§„åˆ™çš„æ–­å¥
        """
        text_length = re.findall("(text.length:)+(.+?)(,)+", text)
        SegLargeThan60_length = re.findall("(SegLargeThan60.length:)+(.+?)(,)+", text)
        SegShortLargeThan60_length = re.findall("(SegShortLargeThan60.length:)+(.+?)(,|)+", text)
        text_length_list = [int(w[1]) for w in text_length]
        SegLargeThan60_length_list = [int(w[1]) for w in SegLargeThan60_length]
        SegShortLargeThan60_length_list = [int(w[1]) for w in SegShortLargeThan60_length]
        total_text = sum(text_length_list)
        ori = sum(SegLargeThan60_length_list)
        after = sum(SegShortLargeThan60_length_list)

        print("total_text:{}, ori:{}, after:{}", total_text, ori, after)
        print("å¯¹é•¿åº¦å¤§äº 60 å°±ç›´æ¥ä¸¢å¼ƒï¼š{}", ori / total_text)
        print("åŸºäºè§„åˆ™çš„æ–­å¥ï¼š{}", after / total_text)

        """

        """
        text_origin = re.findall(r"(##SegSentenceForJieba## text_origin:)+(.+?)(2021-03-12)", text, re.S)
        textListSplit = re.findall(r"(##SegSentenceForJieba## textListSplit:)+(.+?)(2021-03-12)", text, re.S)
        textList = re.findall(r"(##SegSentenceForJieba## textList:)+(.+?)(2021-03-12)", text, re.S)
        data_dict = {}
        data_dict["text_origin"] = [w[1] for w in text_origin]
        data_dict["textListSplit"] = [w[1][1:-2].split(",") for w in textListSplit]
        data_dict["textList"] = [w[1][1:-2].split(",") for w in textList]
        data = pd.DataFrame(data_dict)
        data["check_is_same_length"] = data.apply(lambda row: self.check_is_same_length(
            col1=row["textListSplit"],
            col2=row["textList"]), axis=1)
        data_select = data[(data["check_is_same_length"] == True)]
        data_select["cut_res"] = data_select["text_origin"].apply(lambda x: ss.cut(x))
        # data_select.loc[:, "cut_res"] = data_select[:, "text_origin"].apply(lambda x: ss.cut(x))

        flat_list = [item for sublist in data_select["cut_res"].tolist() for item in sublist]
        # select_list = [w for w in flat_list if 30 <= len(w) < 60]
        res_list = [w for w in flat_list if len(w) > 60]

        print("éœ€è¦ç”¨ç®—æ³•å†æ¬¡åˆ‡åˆ†çš„æ–‡ç« æ•°ä¸ºï¼š{}", len(data_select))
        print("ç”¨ç®—æ³•åˆ‡åˆ†åæ€»çš„å¥å­æ•°é‡ä¸ºï¼š{}", len(flat_list))
        print("ç”¨ç®—æ³•åˆ‡åˆ†åï¼Œé•¿åº¦ä»ç„¶å¤§äº60çš„å¥å­æ•°é‡ä¸ºï¼š{}", len(res_list))
        print("åŸºäºç®—æ³•çš„æ–­å¥ï¼š{}", len(res_list) / total_text)
        # with open("/Users/apple/XHSworkspace/data/210312/res_jieba_list_dayu60.pickle", "w") as f:
        #     f.write("\n".join(res_list))

        return data

    def check_is_same_length(self, col1, col2):
        return True if len(col1) > 0 and len(col1) > len(col2) else False


if __name__ == '__main__':
    ss = SegSentence()
    """test cut one sentence
    """
    # res = ss.cut(
    #     "ç„¶åè¯´æ‰‹æœº1800 è®©æˆ‘å…ˆé¦–ä»˜400ç„¶ååˆ†æœŸ ç„¶åä»–è¯´ä»–å…ˆç»™æˆ‘å¯„è¿‡æ¥ ç¡®å®å•å·ç‰©æµéƒ½æœ‰ ä½†æ˜¯éƒ½æ˜¯å‡çš„ æˆ‘æ²¡æœ‰æŸ¥åˆ°ç‰©æµä¿¡æ¯ ä»–è¿˜ç¡¬è¯´åˆ°äº†äº‹åæ‹¿åˆ°é’±ä¹‹åç›´æ¥ä¸ç†äºº å¾®ä¿¡åˆ äº†å°çº¢ä¹¦ä¹Ÿåˆ äº† å°±QQç•™ç€ QQä¹Ÿä¸çŸ¥é“è®¾ç½®äº†å•¥ åæ­£å‘äº†å¾ˆå¤šæ¶ˆæ¯æ‰“äº†å¾ˆå¤šç”µè¯ç¡¬æ˜¯ä¸æ¥, ä»–è¿˜ç¡¬è¯´åˆ°äº†äº‹åæ‹¿åˆ°é’±ä¹‹åç›´æ¥ä¸ç†äºº å¾®ä¿¡åˆ äº†å°çº¢ä¹¦ä¹Ÿåˆ äº† å°±QQç•™ç€ QQä¹Ÿä¸çŸ¥é“è®¾ç½®äº†å•¥ åæ­£å‘äº†å¾ˆå¤šæ¶ˆæ¯æ‰“äº†å¾ˆå¤šç”µè¯ç¡¬æ˜¯ä¸æ¥")
    # res = ss.cut(
    #     """cosçš„è¡£æœç©¿ç€ç©¿ç€ï¼Œå°±æˆä¸ºcosé‡æ¨¡äº†ï¼ğŸ˜‚
    #
    #     ğŸŒˆ:ä¸Šè£…æ¨è
    #     æ–°æ¬¾åœ†é¢†è¡¬è¡«ï¼Œæ‚å¿—æ¨èæ¬¾ï¼ğŸ‘ˆ
    #     æ‰‹æ„Ÿæ˜¯å¾ˆèˆ’æœçš„æ£‰ï¼Œå¾ˆè–„ï¼Œä¸Šèº«æ„Ÿè§‰å®Œå…¨æ˜¯å¯ä»¥åœ¨å¤å¤©ç©¿çš„ï¼ğŸ¦(æˆ‘è„–å­çŸ­ï¼Œåœ†é¢†å°±æ˜¯æˆ‘çš„è¡£æœäº†ï¼ç•¥é€ï¼Œå…¥æ‰‹è¦å¸¸å¤‡rtï¼)
    #     æ­å·çš„å¤å¤©å¯æ˜¯è¦åˆ°10æœˆä¸­æ—¬æ‰ç»“æŸï¼ç§‹å¤©æ—¶å€™åŠ ä¸ªå¤–å¥—ï¼Œä¼°è®¡ä¹Ÿå¾ˆå¸…ğŸ¤ª
    #
    #     ğŸŒˆ:ä¸‹è£…æ¨è
    #     æ–°æ¬¾çš„è–„å‹ç¾Šæ¯›è£¤(è—é’è‰²çš„)ï¼Œç‰ˆå‹è¶…çº§å¥½ï¼Œç‹‚æ˜¾ç˜¦ï¼
    #     è¥¿è£…è£¤çš„ç±»å‹ï¼Œä¼‘é—²ç©¿é…å°ç™½é‹ï¼Œæ­£è£…å°±é…ä¸ªçš®é‹å•¥çš„ï¼Œè¿™è£¤å­æ˜¯æœ‰è¥¿è£…é…å¥—çš„ï¼(ä¸‹æ¬¡æ›´æ–°å“ˆï¼Œè¶…å¸…ï¼)
    #     âš ï¸:å¤å¤©ç©¿ç¾Šæ¯›ä¸çƒ­å—ï¼Ÿè°¢è°¢ï¼Œcoså®¶åšçš„è–„ç¾Šæ¯›ç³»åˆ—çœŸçš„å¤å¤©å¯ä»¥ç©¿ã€‚å½“ç„¶ï¼Œä¸ºäº†å¥½çœ‹æˆ‘æ„¿æ„å¤©å¤©å¾…ç©ºè°ƒé—´ å˜»å˜».ğŸ¤ª
    #
    #     ğŸŒˆï¼šç‰›çš®å°ç™½é‹
    #     cosç‰›çš®å°ç™½é‹ï¼Œå¹´å¹´å‡ºï¼Œå¹´å¹´å–æ–­è´§ï¼ç°åœ¨åº—é‡Œè¿˜æœ‰åŒæ¬¾é»‘è‰²çš„ï¼Œæˆ‘ä¸ªäººæ›´ä¸­æ„ç™½è‰²çš„å“ˆï¼æˆ‘ç©¿40çš„åˆšå¥½ï¼Œç‰›çš®æ–™å­ï¼Œä¸è´µä¸è´µ.ğŸ¤¤
    #
    #     å¥½çš„ï¼Œæˆ‘è¯´å®Œäº†ğŸ¤ªæœ‰å…´è¶£å¯ä»¥å»äº†è§£ä¸‹ï¼Œè¿˜æœ‰è¯·å¤§å®¶çœ‹å®Œèƒ½èµä¸ªå°å¿ƒå¿ƒâ¤ï¸é èº¬.
    #     Collection of Style[å“ç‰Œ]##è¯•è¡£é—´è‡ªæ‹[è¯é¢˜]##COLLECTION OF STYLE[åœ°ç‚¹]#"""
    # )
    # print(res)
    # lines = read_file("/Users/apple/XHSworkspace/data/structure/food/000000_0.csv_cutDoc_20210929")
    lines = read_file("/Users/apple/XHSworkspace/data/structure/food/15th_1000_1008_pred_fresh")
    for i in tqdm(lines):
        # i = "5f6901e00000000001002d49/t /t åŒ…èœç‚’é¸¡/t /t åŸæ–™:é¸¡è…¿,åŒ…èœ,å¹²é¦™è‡ ,å°è‘±,è’œ,è±†ç“£é…±,ç”ŸæŠ½,ç›,éº»æ²¹/t /t å‡†å¤‡:é¸¡è…¿ç æˆå°å—,è¿‡æ°´åæ´—å»æµ®æ²«;åŒ…èœåˆ‡å¤§å—;å¹²é¦™è‡æå‰æ³¡è½¯,æ–œåˆ€åˆ‡ç‰‡;è‘±è’œåˆ‡æœ«ã€‚ ##SEP## /t /t åšæ³•:æ²¹çƒ­åæ”¾è‘±æœ«çˆ†é¦™,æ”¾è±†ç“£é…±ç‚’å‡ºçº¢æ²¹åæ”¾å…¥ç„¯è¿‡æ°´çš„é¸¡å—å’Œé¦™è‡ç‰‡,ç¨ç¨ç‚’ä¸€ä¸‹åŠ ä¸€ä¸¢ä¸¢å¼€æ°´åŠæ²¡è¿‡é£Ÿæå°±å¥½,åŠ ç”ŸæŠ½æå‘³(å¯ä»¥åŠ ç‚¹è€æŠ½ä¸Šè‰²), ##SEP## ä¸­ "
        # i = "5f8c0c170000000001006772/t /t æ¹–å·ç¾é£Ÿ è‰¯å¿ƒå®è—å¤–å–åº—é“ºæ¨è(ç¬¬äºŒæœŸ)/t /t æœ€çœŸå®çš„ç…§ç‰‡äº†,å› ä¸ºæ‡’å¾—på›¾ã€‚è™½ç„¶æœ‰ç‚¹ä¸‘å¥½åƒå°±è¡Œäº†!è¯·ç›¸ä¿¡æˆ‘å¥½å—?/t å®è—åº—é“ºä¸€:æ‹¼ä¸€ç¢—è›‹åŒ…é¥­/t æ¨è:é¦™æµ“å’–å–±è›‹åŒ…é¥­é…å¥¥å°”è‰¯è…¿æ’/t ##SEP## å‡ä»·:15/t è¯„ä»·:å¾ˆå¥½åƒçš„è›‹åŒ…é¥­,è¿˜é€äº†ä¸€å°è¢‹è‚‰æ¾,åŠ ä¸Šäº†è‚‰æ¾å£æ„Ÿå¾ˆæ£’,é¥­é‡å¾ˆè¶³,ä¸‹é¢çš„é¥­ä¹Ÿå¾ˆé¦™,éšæ„ç‚¹ä¸è¸©é›·/t å®è—åº—é“ºäºŒ:å°å½“å®¶/t ##SEP## æ¨è:å¹²é”…é¸¡å¥—é¤/t å®«ä¿é¸¡ä¸å¥—é¤/t å‡ä»·:11/t è¯„ä»·:ç”¨äº†çº¢åŒ…æ‰åå—å¤š,é‡Œé¢çš„é…èœè¿˜å¯ä»¥éšä¾¿é€‰è¶…çº§è¶…çº§åˆ’ç®—äº†,å‘³é“çœŸçš„å¾ˆèµ, ##SEP## é±¼é¦™è‚‰ä¸å’Œå®«ä¿é¸¡ä¸çš„å£æ„Ÿä¸€æ ·å°±æ˜¯é‡Œé¢çš„è‚‰ä¸ä¸€æ ·,è‚¥ç‰›çš„é‚£ä¸ªä¹Ÿå¾ˆå¥½åƒå“¦!æ€§ä»·æ¯”è¶…é«˜,è¶…çº§æ¨èè¿™å®¶!å°±æ˜¯é…é€æœ‰ç‚¹ä¸å‡†æ—¶å¤§éƒ¨åˆ†è¿˜æ˜¯å‡†æ—¶çš„ã€‚ ##SEP## /t å®è—åº—é“ºä¸‰:è€ä¿æœ¨ç‚­çƒ§é¥¼/t æ¨è:é¦™é…¥é¸¡çƒ§é¥¼/t å‡ä»·:13/t è¯„ä»·:é¦™é…¥é¸¡ä»½é‡å¾ˆè¶³,ä¸€å®šè¦åŠ ç”œé…±ä¸€å®šè¦åŠ ç”œé…±!åƒè¿‡å¢è®°çš„,å°ä¿çš„, ##SEP## è¿˜æ˜¯è§‰å¾—è¿™å®¶æœ€å¥½åƒ!ç¼ºç‚¹é…é€è´¹æœ‰ç‚¹è´µ,å¯ä»¥å’Œå®¤å‹ä¸€èµ·ç‚¹å“ˆå“ˆå“ˆ!å†²é¸­!/t å› ä¸ºä¸åŒçš„èœä»·é’±ä¹Ÿä¸åŒ,å‡ä»·åªèƒ½åšå‚è€ƒ,çº¢åŒ…æ´¥è´´å‡æ‰åä»·æ ¼ä¹Ÿä¸ä¸€æ ·, ##SEP## å…·ä½“çš„å¯ä»¥å»ç¾å›¢æˆ–è€…é¥¿äº†å—è‡ªå·±çœ‹çœ‹å“¦!æ¯ä¸ªäººå£å‘³ä¸åŒ,ä»…ä¾›å‚è€ƒå“¦!å–œæ¬¢çš„ç‚¹ç‚¹å…³æ³¨,å¤–å–ä¸è¿·è·¯!"
        i = i.replace("/t /t ", "/t")
        i = i.replace("/t/t/t ", "/t")
        i = i.replace(" ##SEP## ", " ")
        id = i.split("/t")[0]
        text = i[len(id) + 2:]
        text = text.replace("/t", "\t")
        res = ss.cut(text)
        for jdx, jval in enumerate(res):
            if len(jval) < 10 or len(jval) > 120:
                print("ori_cut: {}".format(text))
                print("new_cut")
                print("index: {}, value: {}".format(jdx, jval))
